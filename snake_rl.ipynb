{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STILL_ALIVE_REWARD = 1\n",
    "DEAD_REWARD = -1000\n",
    "\n",
    "CROP_SHAPE = (750, 539, 1)\n",
    "RESIZE_WIDTH = 180\n",
    "RESIZE_HEIGHT = 137\n",
    "READ_BATCH = 2\n",
    "\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "UP = 4\n",
    "\n",
    "SHOTS_FOLDER = 'data/shots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Browser functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectLevel(l):\n",
    "    xpath = '/html/body/section/div[2]/nav/p[' + str(l+1) + ']'\n",
    "    \n",
    "    level = browser.find_element_by_xpath(xpath)\n",
    "    level.click()\n",
    "    \n",
    "    \n",
    "def clickBoard():        \n",
    "    xpath = '/html/body/section/div[2]/div'\n",
    "    board = browser.find_element_by_xpath(xpath)\n",
    "\n",
    "    try:\n",
    "        board.click()\n",
    "    except WebDriverException:\n",
    "        print('Exception')\n",
    "        return\n",
    "    \n",
    "    \n",
    "def getScreen(fileName):    \n",
    "    state = getState()\n",
    "#     if state == 'playing' or state == 'paused':    \n",
    "    ss = browser.get_screenshot_as_file(SHOTS_FOLDER + fileName)\n",
    "    \n",
    "    \n",
    "def getState():\n",
    "    xpath = '/html/body/section/div[2]'\n",
    "    state = browser.find_element_by_xpath(xpath)\n",
    "    c = state.get_attribute('class')\n",
    "    \n",
    "    return c.split(' ')[-1]\n",
    "\n",
    "\n",
    "def getScore():\n",
    "    state = getState()\n",
    "    \n",
    "    if state == 'playing' or state == 'paused':\n",
    "        \n",
    "        xpath = '/html/body/section/div[2]/p[1]/span'\n",
    "        score = browser.find_element_by_xpath(xpath)\n",
    "\n",
    "        if not score.text.isnumeric():\n",
    "            return 0\n",
    "        return int(score.text)\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "def makeMove(m):\n",
    "    xpath = '/html/body/section/div[2]/div'\n",
    "    board = browser.find_element_by_xpath(xpath)\n",
    "\n",
    "    if m == RIGHT:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.ARROW_RIGHT)\n",
    "    elif m == DOWN:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.ARROW_DOWN)\n",
    "    elif m == LEFT:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.ARROW_LEFT)\n",
    "    elif m == UP:\n",
    "        browser.find_element_by_tag_name('body').send_keys(Keys.ARROW_UP)\n",
    "        \n",
    "\n",
    "def makeRandomMove():\n",
    "    r = random.randint(0, 4)\n",
    "    makeMove(r)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLastGameIndex():\n",
    "    '''Get the index of the last current game.'''\n",
    "    files = os.listdir(SHOTS_FOLDER)\n",
    "    if len(files) == 0:\n",
    "        return 0\n",
    "    \n",
    "    last = files[-1]\n",
    "    \n",
    "    return parseName(last)[0]\n",
    "\n",
    "\n",
    "def getName(g, i, m, r):\n",
    "    '''Form the file name from the board data.'''\n",
    "    return str(g).zfill(3) + '_' + str(i).zfill(3) + '_' + str(m) + '_' + str(r) + '.png'\n",
    "\n",
    "\n",
    "def parseName(fileName):\n",
    "    '''Parse the board data from the file name.'''\n",
    "    return [int(d) for d in fileName[:-4].split('_')]\n",
    "\n",
    "\n",
    "def cropImgOld(fileName):\n",
    "    ss = plt.imread(fileName)\n",
    "\n",
    "    ss = ss[120:659, 273:1023, :]\n",
    "    \n",
    "    plt.imsave(fileName, ss)\n",
    "        \n",
    "\n",
    "def preprocImg(fileName):\n",
    "    '''Preprocess a screenshot.'''\n",
    "    im = Image.open(fileName)    \n",
    "    \n",
    "    # Crop board\n",
    "    im = im.crop((273, 120, 1023, 659))\n",
    "    \n",
    "    # Grayscale\n",
    "    im = ImageOps.grayscale(im)\n",
    "    \n",
    "    # Binarization\n",
    "    t = 127\n",
    "    im = im.point(lambda x: 255 if x > t else 0)\n",
    "    \n",
    "    # Resize\n",
    "    im = im.resize((RESIZE_WIDTH, RESIZE_HEIGHT))\n",
    "    \n",
    "    return im\n",
    "    \n",
    "    \n",
    "def preprocAll(gameIndex):\n",
    "    '''Preprocess all screenshots in the data folder.'''\n",
    "    files = os.listdir(SHOTS_FOLDER)\n",
    "    for f in files:\n",
    "        g = parseName(f)[0]\n",
    "        \n",
    "        if g >= gameIndex:        \n",
    "            fileName =  SHOTS_FOLDER + f\n",
    "            im = preprocImg(fileName)\n",
    "            im.save(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocAll(86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://playsnake.org/'\n",
    "\n",
    "browser = webdriver.Chrome(executable_path='D:/Libraries/Drivers/chromedriver_win32/chromedriver.exe')\n",
    "# browser = webdriver.Chrome(executable_path='D:/Biblioteci/Python/chromedriver_win32/chromedriver.exe')\n",
    "browser.get(url) \n",
    "\n",
    "START_GAME = getLastGameIndex() + 1\n",
    "gameIndex = START_GAME\n",
    "\n",
    "maxGames = 100\n",
    "\n",
    "for g in range(maxGames):\n",
    "    \n",
    "    # Select the level\n",
    "    selectLevel(1)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    score = 0\n",
    "    screenIndex = 0\n",
    "    \n",
    "    clickBoard()\n",
    "    while getState() == 'playing' or getState() == 'paused':\n",
    "        s2 = getScreen('prev.png')\n",
    "        screenIndex += 1    \n",
    "\n",
    "        clickBoard()\n",
    "        m = makeRandomMove()\n",
    "        time.sleep(0.1)\n",
    "        clickBoard()\n",
    "        \n",
    "        # Update last board  \n",
    "        r = max(STILL_ALIVE_REWARD, getScore() - score) \n",
    "        fileName = getName(gameIndex, screenIndex, m, r)\n",
    "        os.rename(SHOTS_FOLDER + 'prev.png', SHOTS_FOLDER + fileName)\n",
    "        score = getScore()\n",
    "        \n",
    "    # Change reward of last board\n",
    "    g, i, m, r = parseName(fileName)\n",
    "    os.rename(SHOTS_FOLDER + fileName, SHOTS_FOLDER + getName(g, i, m, DEAD_REWARD))\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    gameIndex += 1\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "preprocAll(START_GAME)\n",
    "\n",
    "# RIGHT = 1\n",
    "# DOWN = 2\n",
    "# LEFT = 3\n",
    "# UP = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "import keras.layers\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, Input\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, Adadelta, RMSprop\n",
    "import keras.losses as losses\n",
    "from keras.backend import set_image_data_format\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    files = os.listdir(SHOTS_FOLDER)\n",
    "    \n",
    "    labels = np.zeros(len(files))    \n",
    "    data = []\n",
    "    \n",
    "    for i, f in enumerate(files):\n",
    "        im = Image.open(SHOTS_FOLDER + f)\n",
    "        \n",
    "        if f in history:\n",
    "            labels[i] = history[f][2]\n",
    "        else:\n",
    "            labels[i] = STILL_ALIVE_REWARD\n",
    "            \n",
    "        data.append([np.asarray(im.convert(\"L\"))])\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def sampleData(nSamples, propD, propR, propA):\n",
    "    '''Sample the input data according to some proportions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nSamples: total number of samples\n",
    "    propD: proportion of samples which represent a death\n",
    "    propR: proportion of samples which represent a reward\n",
    "    propA: proportion of rest of samples\n",
    "    '''\n",
    "    \n",
    "    files = os.listdir(SHOTS_FOLDER)\n",
    "    \n",
    "    perc = []\n",
    "    nd, na, nr = (0, 0, 0)\n",
    "    for f in files:\n",
    "        r = parseName(f)[3]\n",
    "        if r == DEAD_REWARD:\n",
    "            perc.append('d')\n",
    "            nd += 1\n",
    "        elif r == STILL_ALIVE_REWARD:\n",
    "            perc.append('a')\n",
    "            na += 1\n",
    "        else:\n",
    "            perc.append('r')\n",
    "            nr += 1\n",
    "        \n",
    "    n = propD * nd + propA * na + propR * nr\n",
    "    pd = propD / n\n",
    "    pa = propA / n\n",
    "    pr = propR / n\n",
    "    for i in range(len(perc)):\n",
    "        if perc[i] == 'd':\n",
    "            perc[i] = pd\n",
    "        elif perc[i] == 'a':\n",
    "            perc[i] = pa\n",
    "        else:\n",
    "            perc[i] = pr\n",
    "        \n",
    "    samples = np.random.choice(files, size=nSamples, replace=False, p=perc)\n",
    "    print('Sampled data.')\n",
    "    return samples\n",
    "\n",
    "\n",
    "def readSamplesBasic(samples):\n",
    "    '''Read samples for basic model, which just classifies if a board is finished or not.'''\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in samples:\n",
    "        im = Image.open(SHOTS_FOLDER + file)\n",
    "        g, i, m, r = parseName(file)\n",
    "        \n",
    "        data.append([np.asarray(im.convert(\"L\"))])\n",
    "        labels.append(1 if r > 0 else 0)\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def getPreviousSample(file):\n",
    "    '''Get the name of the previous sample.'''\n",
    "    \n",
    "    g, i, m, r = parseName(file)\n",
    "    i -= 1\n",
    "    \n",
    "    if i > 0:        \n",
    "        prevFile = glob.glob(SHOTS_FOLDER + str(g).zfill(3) + '_' + str(i).zfill(3) + '*')[0]\n",
    "        prevFile = prevFile.replace('\\\\', '/')\n",
    "        prevFile = prevFile.split('/')[-1]\n",
    "#         print(prevFile)\n",
    "        return prevFile      \n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "def readSample(file):\n",
    "    '''Read a single sample as an array.'''\n",
    "    im = Image.open(SHOTS_FOLDER + file)\n",
    "    g, i, m, r = parseName(file)\n",
    "\n",
    "    data = np.asarray(im.convert(\"L\"))\n",
    "    label = 1 if r > 0 else 0\n",
    "    \n",
    "    return data, label\n",
    "    \n",
    "    \n",
    "def readSampleMove(file):\n",
    "    '''Read a single sample as an array, also returns the move.'''\n",
    "    im = Image.open(SHOTS_FOLDER + file)\n",
    "    g, i, m, r = parseName(file)\n",
    "\n",
    "    data = np.asarray(im.convert(\"L\"))\n",
    "#     if r == -1000:\n",
    "#         label = -1\n",
    "#     elif r > 1:\n",
    "#         label = r / 100\n",
    "#     else:\n",
    "#         label = 0\n",
    "    label = r\n",
    "    \n",
    "    return data, m, label\n",
    "\n",
    "\n",
    "def readBatchSamples(samples, batch):\n",
    "    '''Read samples in batches, so that we also have information about the movement.'''\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in samples:        \n",
    "        dataBatch = []\n",
    "        \n",
    "        d, label = readSample(file)\n",
    "        dataBatch.append(d)\n",
    "        \n",
    "        # Get previous files\n",
    "        prevFile = file\n",
    "        for p in range(batch - 1):\n",
    "            prevFile = getPreviousSample(prevFile)\n",
    "            \n",
    "            if prevFile == -1:\n",
    "                break\n",
    "                \n",
    "            d, _ = readSample(prevFile)\n",
    "            dataBatch.append(d)\n",
    "        \n",
    "        if prevFile != -1:\n",
    "            dataBatch.reverse()\n",
    "            data.append(dataBatch)\n",
    "            labels.append(label)\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    return data, labels   \n",
    "\n",
    "\n",
    "def readBatchSamplesMove(samples, batch):\n",
    "    '''Read samples in batches, so that we also have information about the movement.\n",
    "    Also reads the moves.'''\n",
    "    data = []\n",
    "    actions = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, file in enumerate(samples):        \n",
    "        dataBatch = []\n",
    "        \n",
    "        d, a, r = readSampleMove(file)\n",
    "        dataBatch.append(d)\n",
    "        \n",
    "        # Get previous files\n",
    "        prevFile = file\n",
    "#         print(file)\n",
    "        for p in range(batch - 1):\n",
    "            prevFile = getPreviousSample(prevFile)\n",
    "#             print('>', prevFile)\n",
    "            \n",
    "            if prevFile == -1:\n",
    "                break\n",
    "                \n",
    "            d, _ = readSample(prevFile)\n",
    "            dataBatch.append(d)\n",
    "        \n",
    "        if prevFile != -1:\n",
    "            dataBatch.reverse()\n",
    "            data.append(dataBatch)\n",
    "            actions.append(a)\n",
    "            labels.append(r)\n",
    "#             print('->', a, r)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    actions = np.asarray(actions)\n",
    "    return data, actions, labels    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_model_actionlast()\n",
    "data, actions, labels = readBatchSamplesMove(sampleData(1, 1, 10, 0.5), READ_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN basic\n",
    "\n",
    "Sequential, citeste doar o imagine, nu considera actiunea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_basic():\n",
    "    '''NN model for basic test.'''\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (4, 4), input_shape=(1, RESIZE_HEIGHT, RESIZE_WIDTH), activation='relu', name='conv1'))\n",
    "    model.add(Conv2D(16, (4, 4), activation='relu', name='conv2'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool1'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(8, (4, 4), activation='relu', name='conv3'))\n",
    "    model.add(Conv2D(8, (4, 4), activation='relu', name='conv4'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool2'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(16, (4, 4), activation='relu', name='conv5'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool3'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten(name='flat'))\n",
    "    model.add(Dense(100, activation='relu', name='dense1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(40, activation='relu', name='dense2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer = 'rmsprop',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data.\n",
      "(200, 1, 137, 180)\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Epoch 1/7\n",
      " 40/200 [=====>........................] - ETA: 28s - loss: 7.2681 - acc: 0.5250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6237bbb91df3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = nn_model_basic()\n",
    "data, labels = readSamplesBasic(sampleData(200, 10, 0.5, 0.5))\n",
    "print(data.shape)\n",
    "print(labels)\n",
    "\n",
    "model.fit(data, labels, epochs=7, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN seq\n",
    "\n",
    "Sequential, citeste batch-uri de imagini, dar nu considera si actiunea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model_seq():\n",
    "    '''Sequential NN model.'''\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, (4, 4), input_shape=(READ_BATCH, RESIZE_HEIGHT, RESIZE_WIDTH), activation='relu', name='conv1'))\n",
    "    model.add(Conv2D(16, (4, 4), activation='relu', name='conv2'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool1'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(8, (4, 4), activation='relu', name='conv3'))\n",
    "    model.add(Conv2D(8, (4, 4), activation='relu', name='conv4'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool2'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(16, (4, 4), activation='relu', name='conv5'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), name='pool3'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten(name='flat'))\n",
    "    model.add(Dense(100, activation='relu', name='dense1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(40, activation='relu', name='dense2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer = 'rmsprop',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_model_seq()\n",
    "data, labels = readBatchSamples(sampleData(200, 10, 0.5, 0.5), READ_BATCH)\n",
    "print(data.shape)\n",
    "print(labels)\n",
    "\n",
    "model.fit(data, labels, epochs=7, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN action input\n",
    "\n",
    "Functional API, citeste batch-uri de imagini, considera actiunea ca un input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model_actionlast():\n",
    "    '''Full NN model, using the Keras Functional API.\n",
    "    Action as input, added before the final layer.'''\n",
    "    screen_input = Input(shape=(READ_BATCH, RESIZE_HEIGHT, RESIZE_WIDTH), name='screen_input')\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv1')(screen_input)\n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv2')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool1')(conv)\n",
    "    \n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv3')(pool)\n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv4')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool2')(conv)\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv5')(pool)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool3')(conv)\n",
    "    \n",
    "    flat = Flatten(name='flat')(pool)\n",
    "    dense = Dense(100, activation='relu', name='dense1')(flat)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "    dense = Dense(40, activation='relu', name='dense2')(drop)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "    \n",
    "    action_input = Input(shape=(1,), name='action_input')\n",
    "    concat = keras.layers.concatenate([drop, action_input])\n",
    "    \n",
    "    output = Dense(1, activation='linear', name='output')(concat)\n",
    "    \n",
    "    model = Model(inputs=[screen_input, action_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer = 'rmsprop',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def nn_model_actionmid():\n",
    "    '''Full NN model, using the Keras Functional API.\n",
    "    Action as input, added after convolutional layers.'''\n",
    "    screen_input = Input(shape=(READ_BATCH, RESIZE_HEIGHT, RESIZE_WIDTH), name='screen_input')\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv1')(screen_input)\n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv2')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool1')(conv)\n",
    "    \n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv3')(pool)\n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv4')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool2')(conv)\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv5')(pool)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool3')(conv)\n",
    "    \n",
    "    flat = Flatten(name='flat')(pool)\n",
    "    \n",
    "    action_input = Input(shape=(1,), name='action_input')\n",
    "    concat = keras.layers.concatenate([flat, action_input])\n",
    "    \n",
    "    dense = Dense(100, activation='relu', name='dense1')(concat)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "    dense = Dense(40, activation='relu', name='dense2')(drop)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "        \n",
    "    output = Dense(1, activation='linear', name='output')(drop)\n",
    "    \n",
    "    model = Model(inputs=[screen_input, action_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer = 'rmsprop',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data.\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "(2718, 2, 137, 180)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2718/2718 [==============================] - 441s 162ms/step - loss: 10.5510 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2718/2718 [==============================] - 477s 176ms/step - loss: 10.2389 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2718/2718 [==============================] - 466s 171ms/step - loss: 11.7683 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2718/2718 [==============================] - 420s 155ms/step - loss: 9.1917 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2718/2718 [==============================] - 510s 188ms/step - loss: 7.8799 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2718/2718 [==============================] - 407s 150ms/step - loss: 7.0826 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2718/2718 [==============================] - 417s 153ms/step - loss: 5.9229 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2718/2718 [==============================] - 406s 149ms/step - loss: 7.4059 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2718/2718 [==============================] - 476s 175ms/step - loss: 6.3856 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2718/2718 [==============================] - 461s 170ms/step - loss: 5.3731 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4245c2cf8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn_model_actionlast()\n",
    "data, actions, labels = readBatchSamplesMove(sampleData(3000, 5, 5, 1), READ_BATCH)\n",
    "print(data.shape)\n",
    "\n",
    "labels = np.asarray(labels).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(labels)\n",
    "labels = scaler.transform(labels)\n",
    "\n",
    "model.fit([data, actions], labels, epochs=10, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNN action output\n",
    "\n",
    "Functional API, citeste batch-uri de imagini, considera actiunea ca output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model_actionlast():\n",
    "    '''Full NN model, using the Keras Functional API.\n",
    "    Action as input, added before the final layer.'''\n",
    "    screen_input = Input(shape=(READ_BATCH, RESIZE_HEIGHT, RESIZE_WIDTH), name='screen_input')\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv1')(screen_input)\n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv2')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool1')(conv)\n",
    "    \n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv3')(pool)\n",
    "    conv = Conv2D(8, (4, 4), activation='relu', name='conv4')(conv)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool2')(conv)\n",
    "    \n",
    "    conv = Conv2D(16, (4, 4), activation='relu', name='conv5')(pool)\n",
    "    pool = MaxPool2D(pool_size=(2,2), name='pool3')(conv)\n",
    "    \n",
    "    flat = Flatten(name='flat')(pool)\n",
    "    dense = Dense(100, activation='relu', name='dense1')(flat)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "    dense = Dense(40, activation='relu', name='dense2')(drop)\n",
    "    drop = Dropout(0.5)(dense)\n",
    "    \n",
    "    action_input = Input(shape=(1,), name='action_input')\n",
    "    concat = keras.layers.concatenate([drop, action_input])\n",
    "    \n",
    "    output = Dense(1, activation='linear', name='output')(concat)\n",
    "    \n",
    "    model = Model(inputs=[screen_input, action_input], outputs=output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer = 'rmsprop',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_model_actionlast()\n",
    "data, actions, labels = readBatchSamplesMove(sampleData(600, 5, 5, 1), READ_BATCH)\n",
    "print(data.shape)\n",
    "\n",
    "labels = np.asarray(labels).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(labels)\n",
    "labels = scaler.transform(labels)\n",
    "\n",
    "model.fit([data, actions], labels, epochs=5, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-683.5772 ]\n",
      " [-682.5713 ]\n",
      " [-679.87006]\n",
      " [-681.6437 ]\n",
      " [-682.2808 ]\n",
      " [-681.5002 ]\n",
      " [-679.9691 ]\n",
      " [-678.7517 ]\n",
      " [-679.8273 ]\n",
      " [-685.2819 ]\n",
      " [-682.4278 ]\n",
      " [-675.132  ]\n",
      " [-679.9722 ]\n",
      " [-679.36163]\n",
      " [-676.61444]\n",
      " [-681.62366]\n",
      " [-681.1422 ]\n",
      " [-680.25006]\n",
      " [-676.3751 ]\n",
      " [-677.0779 ]]\n",
      "[[ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [ 0.56887387]\n",
      " [ 0.55952571]\n",
      " [ 0.55952571]\n",
      " [-1.77985056]\n",
      " [-1.77985056]\n",
      " [ 0.55952571]]\n"
     ]
    }
   ],
   "source": [
    "# model.evaluate(data[30:50], labels[30:50])\n",
    "\n",
    "print(model.predict([data[30:50], actions[30:50]]))\n",
    "print(labels[30:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 2, 137, 180)\n",
      "(193, 1)\n",
      "(193, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 193]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-c6b1f0fce75e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0msKFold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Results: %.5f (%.5f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 193]"
     ]
    }
   ],
   "source": [
    "# Build pipeline\n",
    "\n",
    "layers = []\n",
    "layers.append(('nn', KerasClassifier(build_fn=nn_model_actionlast,\n",
    "                                   epochs=5,\n",
    "                                   batch_size=10,\n",
    "                                   verbose=1)))\n",
    "estimator = Pipeline(layers)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "data, actions, labels = readBatchSamplesMove(sampleData(200, 10, 0.5, 0.5), 2)\n",
    "\n",
    "# print(labels)\n",
    "# labels = np.asarray(labels).reshape(-1, 1)\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(labels)\n",
    "# labels = scaler.transform(labels)\n",
    "# print(labels.reshape(labels.size))\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "labels = labels.reshape((labels.size, 1))\n",
    "actions = actions.reshape((actions.size, 1))\n",
    "print(data.shape)\n",
    "print(actions.shape)\n",
    "print(labels.shape)\n",
    "     \n",
    "sKFold = StratifiedKFold(n_splits=2)\n",
    "results = cross_val_score(estimator, [data, actions], labels, cv=sKFold, verbose=1, scoring='accuracy')\n",
    "\n",
    "print(\"Results: %.5f (%.5f)\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
